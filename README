1. speaker verification model
    - cd deep_speaker
    - CUDA_VISIBLE_DEVICES=1 python train.py

1. synthesizer without feedback control:
    - extract audio feature using process_audio.ipynb (remember to split validation set and test set out for performance evaluation)
    - extract embedding using script deep_speaker/get_gvector.ipynb
    - train baseline multispeaker TTS system using synthesizer_train.py (CUDA_VISIBLE_DEVICES=1 python synthesizer_train.py vctk datasets/vctk/synthesizer)
    - validate and synthesize result using syn.ipynb at any time during training
    - using pre-trained WaveRNN vocoder in vocoder/pretrained, or train new one using vocoder_train.py. (CUDA_VISIBLE_DEVICES=1 python vocoder_train.py -g --syn_dir datasets/vctk/synthesizer  vctk datasets/vctk) For better performance, please use GTA Mel-spectrogram obtained by vocoder_preprocess.py. 

2. synthesizer with feedback constraint: 
    - load two model (speaker verification model and multispeaker synthesizer) by changing the path in feedback_synthesizer/hparams.py
    - CUDA_VISIBLE_DEVICES=1 python fc_synthesizer_train.py
    - evaluate with feedback_syn.ipynb


